[{"categories":null,"contents":"Inspired by a lecture of Prof. Wei Ji Ma , I wrote this introduction to Bayesian updating for the course Behavioral Theory (University of Zurich, Fall 2019). What you know already about Bayes\u0026rsquo; rule Bayes\u0026rsquo; Theorem shows the relationship between a conditional probability and its counterpart. Consider two events $A $ and $ B $, Bayes\u0026rsquo; theorem states that, if $ P(B) \\ne 0 $,\n$$ P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}. $$\n$ P(A) $ and $ P(B) $ are the probabilities of observing $ A $ and $ B $. $ P(A \\mid B) $ is a conditional probability: the probability of event $ A $ occurring given that $ B $ occurs. $ P(B\\mid A) $ is also a conditional probability: the probability of event $ B $ occurring given that $ A $ occurs. Warm-up Exercise You come back from a vacation in an exotic place. You decide to go to the doctor to get tested for a disease common in that exotic area. But you know that medical results are not always accurate. Given the positive test result, what is the probability that you actually have this disease?\nTo formalize, denote the event that a patient has the disease as $D$ and the event the patient is healthy as $H$; let the event that the test turned out positive $+$ or negative $-$ for the disease. Bayes\u0026rsquo; Theorem helps you to compute the probability $ \\Pr(D \\mid +) $ given some other information: To determine the answer to this question you need to know: (i) incidence of the disease, that is the prior probability of the disease in the population, $ \\Pr(D)$ ; (ii) test accuracy:\nprobability of a false positive, $ \\Pr(+ \\mid H)$: how often does it report a positive result for someone without the disease? probability of a false negative, $ \\Pr(- \\mid D)$: how often does the test report a negative result for a sick patient? Assume $ \\Pr(D) = 10\\%, \\Pr(+ \\mid H) = 5\\%, $ and $ \\Pr(- \\mid D) = 20\\% $.\n(a) Draw in the figure the probabilities.\nsolution Priors\nTest Accuracy\nHow do you calculate the posterior probability of having the disease given a positive result of the test?\nUse Bayes\u0026rsquo; Theorem: $$ \\Pr(D \\mid + ) = \\frac{\\Pr(+ \\mid D) \\Pr(D)}{\\Pr(+)} !$$\nFirst, we determine the overall probability of a positive result (the denominator). To do so, we use the Law of Total Probability: for every event $ A $ and finite set of events $ B_n $, $ \\Pr(A) = \\sum_n \\Pr (A \\cap B_n) = \\sum_n \\Pr(A \\mid B_n) \\Pr(B_n)$.\n(b) Calculate the probability of a positive result and the probability of a negative result.\nsolution Use the Total Law of Probability: $\tP(+) = P(+ \\mid H) P(H) + P(+ \\mid D) P(D) = 5\\% \\times 90\\% + 80\\% \\times 10\\% = 12.5\\%. $ Then, $ \\ P(-) = 1 - 12.5\\% = 87.5\\% $.\nYou are ready to calculate the posterior probabilities.\n(c) Calculate the probability of being sick once observed a positive result, $ P(D \\mid +) $.\nsolution $$ \\Pr(D \\mid + )\t= \\frac{P(+ \\mid D) P(D)}{P(+)} = \\frac{80\\% \\times 10\\%}{12.5\\%} = 64\\% $$ There is a 64% chance that someone with a positive test is actually sick.\n(d) Which other probabilities can you recover from this? Which one you need to calculate by Bayes\u0026rsquo; Rule?\nsolution We can directly recover $\\Pr(H \\mid +) = 1- \\Pr(D \\mid + ) = 36\\%.$ One need to calculate the conditional probability on a negative test by Bayes\u0026rsquo; Rule.\n(e) Calculate the remaining conditional probabilities solution $$ \\Pr(D \\mid - ) = \\frac{\\Pr(- \\mid D) \\Pr(D)}{\\Pr(-)} = \\frac{20\\% \\times 10\\%}{ 87.5\\% } = 2.3\\% \\ \\Pr(H \\mid -) = 1 - 2.3\\% = 97.7 $$\nTo summarize:\nPosterior\rNegative\rPositive\rHealthy\r97.7%\r36%\rDisease\r2.3%\r64%\r(f) In the figure (source), assign the following label to the appropriate quantities: priors, test accuracy, probability of the results, and posteriors. solution Priors, $ \\Pr(D) $ and $ \\Pr(H) $: respectively, red dots and blue dots in the population; Test accuracy, $ \\Pr(+ \\mid H) $ and $ \\Pr(- \\mid H) $: respectively, blue bar and red bar in the test line; Probability of result, $ \\Pr(+) $ and $ \\Pr(-) $: respectively, all dots in the right column and all dots in the left column; Posteriors: $ \\Pr(D \\mid + ) $: all red dots in the right column; $ \\Pr(H \\mid + ) $: all blue dots in the right column; $ \\Pr(D \\mid - ) $: all red dots in the left column; $ \\Pr(H \\mid - ) $: all blue dots in the left column. More on Bayes\u0026rsquo; Theorem One of the most important application of Bayes\u0026rsquo; Theorem is inference: it allows us to put probability values on unknown parameter. Consider a random variable $x$ which is distributed conditional on an unknown parameter $ \\theta \\in \\Theta $. You observe some realizations $ x_{obs} $. Using these observations, we are interested in learning more about the true value of $ \\theta $. One interpretation is that this information tells you something about the model behind data.\nTo make this inference, you need to start from a prior distribution on the unknown parameter $\\theta$, that is $ \\Pr(\\theta) $ for each value $ \\theta \\in \\Theta $. This summarize the observer\u0026rsquo;s available information (or lack of information) about the parameter. Moreover, you need a likelihood function that tells you how to interpret the observations, absent prior knowledge. Formally, it is equal to the probability of the observed data under a parameter realization: $ \\Pr (x \\mid \\theta ).$\nExample of conditional distribution\nHowever, since the parameter is unknown and we can fix $ x $ using $ x_{obs} $, we treat the likelihood as a function of $ \\theta $: it expresses the plausibility of different parameter values for a given sample of data. A Bayesian observer computes a posterior distribution of $ \\theta $ given the observed value. In general, the posterior $\\Pr(\\theta \\mid x_{obs})$ is the probability density function over the unknown $\\theta$ given the observations $x_{obs}$, calculated by Bayes\u0026rsquo; rule as $$ \\Pr(\\theta \\mid x_{obs}) = \\frac{ \\Pr (x_{obs} \\mid \\theta)\\Pr(\\theta)}{\\Pr(x_{obs})} $$\nBayes\u0026rsquo; Theorem updates the information on $\\theta$ by extracting the information on $\\theta$ contained in the observation $x_{obs}$. So we can use this result to choose the best among a set of hypotheses as follows: each possible value of the parameter is a hypothesis, and the likelihood of that is the observer\u0026rsquo;s belief that the observations would arise under that hypothesis; priors and posteriors are both belief distributions whose arguments are hypothesis; we should select the hypothesis with the highest posterior to make our choices.\nLast, it is very useful to note that the posterior is proportional to the product of prior and likelihood (denominator is a normalization factor independent of $\\theta$): $$ \\Pr(\\theta \\mid x_{obs}) \\propto \\Pr (x_{obs} \\mid \\theta)\\Pr(\\theta). $$\nGo back to point (a) of the previous exercise. Draw the four products of prior and likelihood and check that it is suggestive of the final posterior.\nsolution Exercise: Competing hypotheses You observe bouncy balls all moving downward. You only consider two possible scenarios:\nScenario 1: all balls are part of the same object, thus they always move together. They move together either up or down, each with probability 0.5. Scenario 2: Each ball is an object by itself and independently moves either up or down, each with probability 0.5. Considering only four bouncy balls, you want to learn which of the following story is true. Note that in this example you only care about direction, whereas speed and position do not play a role in this problem.\nLikelihoods The likelihood of a scenario is the probability of the observations under that scenario: $ \\Pr( obs \\mid Scenario ).$\n(a) What is the likelihood of Scenario 1?\nsolution $\\Pr(obs \\mid Scenario 1) = 0.5 = 1/2.$\n(b) What is the likelihood of Scenario 2?\nsolution $ \\Pr(obs \\mid Scenario 2) = (0.5)^{4} = 1/16.$\n(c) Do the likelihoods of the scenarios sum to one?\nsolution No, there is no law of probability that says that probabilities of same observations under different scenarios sum up to one.\n(d) What is wrong with the phrase \u0026ldquo;the likelihood of the observations\u0026rdquo;?\nsolution One cannot speak about the likelihood of something without an considering a scenario to condition on. Remember that a likelihood is a conditional probability.\nPriors In many applications of interest, priors are not well-defined. In this examples, assume that Scenario 1 occurs twice as often as Scenario 2. You can use these frequencies of occurrence as prior probabilities, reflecting expectations in the absence of specific observations.\n(e) What are the prior probabilities of Scenario 1 and 2?\nsolution $ \\Pr(Scenario 1) = 2/3, \\quad \\Pr(Scenario 2)= 1/3. $\nNormalization Factor As an intermediate step, you need to calculate the product of the likelihood of a scenario multiplied by its prior probability: $ \\Pr(obs \\mid Scenario) \\times \\Pr(Scenario).$\nThese are used to calculate the numerator in Bayes\u0026rsquo; rule.\n(f) Calculate this product for Scenario 1 and 2?\nsolution $ \\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) = 2/3 \\times 1/2 = 1/3 $\n$ \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 ) = 1/3 \\times 1/16 = 1/48 $\n(g) Sum the two products. Is it equal to one?\nsolution No: $ \\quad \\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 ) = 1/3 + 1/48 = 17/48 $.\nThe normalization factor in Bayes\u0026rsquo; Rule is the overall probability of the observations and it is sum of the two products, as calculated above. This is because posteriors have to sum up to one.\nPosteriors (h) Divide the product of prior and likelihood by the normalization factor for each scenario\nsolution $$\t\\Pr(Scenario 1 \\mid obs) = \\frac{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) }{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 )} = \\frac{1/3}{17/48}=\\frac{1}{3} \\times \\frac{48}{17}= \\frac{16}{17}. $$\n$$ \\Pr(Scenario 2 \\mid obs) = \\frac{\\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario 2 ) }{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 )} = \\frac{1/48}{17/48}= \\frac{1}{48} \\times \\frac{48}{17}= \\frac{1}{17}.$$\nYou have just applied Bayes\u0026rsquo; rule to obtain the posteriors for each scenario. Now you want to pick scenario with the highest posterior.\n(i) Which scenario has the highest posterior?\nsolution Scenario 1 is more likely given our observations. Indeed, the brain has a tendency to group the dots together because of their common motion, and perceive them as a single object. This is called Gestalt principle of common fate.\nYou have just explained a well-known phenomena by Bayes inference!\nExercise: Multiple Inputs Consider the following model: your utility of breakfast is given by the product of quality coffee and the quality of the cake you consume: $$ U(\\mbox{breakfast})= \\mbox{quality of cake} \\times \\mbox{quality of coffee}. $$ Assume that the quality is evaluated between 0 and 1.\nThis morning you had a very bad breakfast, with utility of breakfast today 0.2. You want to understand the cause of that. You are better at assessing separately the quality of coffee, but not the quality of cake.\n(a) Suppose further that you hypothesized the quality of the coffee to be 1 (very good coffee). Under this hypothesis, calculate what the quality of the cake must have been.\nsolution $$ \\underbrace{U(\\mbox{breakfast})}_{0.2} = \\mbox{quality of cake} \\times \\underbrace{\\mbox{quality of coffee}}_1 $$\nSo the quality of cake must have been 0.2.\n(b) What if you suppose that the quality of coffee to be 0.4?\nsolution $$ \\underbrace{U(\\mbox{breakfast})}_{0.2} = \\mbox{quality of cake} \\times \\underbrace{\\mbox{quality of coffee}}_{0.4} $$\rSo the quality of cake must have been 0.5.\n(c) Why utility of breakfast provides ambiguous information about quality of the cake?\nsolution Because quality of coffee modulates the utility of breakfast together with the quality of cake and you do not have accurate information on that.\n(d) By going through a few more examples like the ones in (a) and (b), draw on the two-variable likelihood diagram all combinations of hypothesized quality of coffee and hypothesized quality of cake that could have produced the utility of breakfast equal to 0.2. solution (e) Suppose that you have a strong prior that quality of coffee is uniformly distributed between 0.2 and 0.4 for sure. In the two-variable prior diagram, shade the area corresponding to the support of the prior.\nsolution (f) Can we infer something about the quality of cake? Finally draw where the posterior probability is high. solution You can just restricted your posterior. If you believe that the quality of coffee is between 0.2 and 0.5, it means that the quality of cake was above 0.5: the cake was good!\nPuzzle: Monty Hall Paradox You\u0026rsquo;re on a TV show. You\u0026rsquo;re given the choice of three doors: behind one door is a car (prize); behind the others, goats (non-prize). Which one would you pick? Suppose you pick door No.1. Now the host, who knows what\u0026rsquo;s behind the doors, does not open the one you selected but opens another door, say No. 3, which has a goat. Then the host says to you, ``Do you want to pick door No. 2?\u0026quot;\nDoes it matter if you switch? Yes! This is a famous counter-intuitive problem. Let\u0026rsquo;s use Bayes\u0026rsquo; Rule to solve it.\nDenote the event \u0026ldquo;door $ i $ hides a car\u0026rdquo; with $ C_i $.\n(a) When the host of the TV show gives you the choice to pick a door, what is your prior probability of finding the car behind each door?\nsolution Since you do not have any information, you believe each door is equally likely to hide the car: $ \\Pr(C_1) = \\Pr(C_2) = \\Pr(C_3) = \\frac{1}{3}.$\nDenote the event \u0026ldquo;door $ i $ is chosen by the guest\u0026rdquo; with $ X_i $ and the event \u0026ldquo;door $ i $ is opened\u0026rdquo; with $ O_i $. Your priors are unchanged after the host opens a door, by independence between the event $ C_i $ and $ X_i $ (the player doesn\u0026rsquo;t know where is the car in order to make a choice). This implies that $ \\Pr(C_i, X_i) = \\Pr(C_i) \\times \\Pr(X_i)$.\nInitially you have chosen door No. 1. Now the host will open a door that does not hide the car.\n(b) What is the probability that the host will open door No. 3 in the following cases?\nsolution The car is behind door No. 1: $ \\Pr(O_3 \\mid X_1, C_1) = \\frac{1}{2}.$ The car is behind door No. 2: $ \\Pr(O_3 \\mid X_1, C_2) = 1.$ The car is behind door No. 3: $ \\Pr(O_3 \\mid X_1, C_3) = 0.$ (c) What is the overall probability that the host will open door no.3?\nsolution Using total law of probability: $$ \\Pr(O_3 \\mid X_1) = \\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1) + \\Pr(O_3 \\mid X_1, C_2) \\times \\Pr(C_2) + \\Pr(O_3 \\mid X_1, C_3) \\times \\Pr(C_3) = \\frac{1}{2} \\times \\frac{1}{3} + 1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} = \\frac{1}{2}.$$\nThe host opens door No.3. Now we know that $ \\Pr(C_3 \\mid 0_3) = 0 $ (posterior probability).\n(d) Use Bayes\u0026rsquo; rule to calculate the conditional probability that the car is behind the door you chose, given that door No.3 was open: $ \\Pr(C_1 \\mid X_1, O_3) $. solution $$\\Pr(C_1 \\mid X_1, O_3) = \\frac{\\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1, X_1)}{\\Pr(X_1, O_3)} = \\frac{\\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1) \\times \\Pr(X_1)}{\\Pr(O_3 \\mid X_1)\\times \\Pr(X_1)} = \\frac{1/2 \\times 1/3 \\times 1}{1/2 \\times 1} = \\frac{1}{3} $$\n(e) Calculate the conditional probability that the car is behind the other door.\nsolution This is just the complement of $\\Pr(C_2 \\mid X_1, O_3) $, since door No.3 was open.\n$$\\Pr(C_2 \\mid X_1, O_3) = 1- \\Pr(C_2 \\mid X_1, O_3) = \\frac{2}{3} $$\n(f) Should you switch door?\nsolution Yes!\n","permalink":"https://chiaraaina.github.io/more/bayesian_updating/","tags":null,"title":"bayesian updating"},{"categories":null,"contents":"a bunch of resources about post-truth that I read, watched, listened\nbooks \u0026ldquo;Nothing is true, everything is possible\u0026rdquo; Peter Pomerantsev (2015) PublicAffairs \u0026ldquo;Post-Truth\u0026rdquo; Lee Mcintyre (2018) MIT Press \u0026ldquo;This is not propaganda\u0026rdquo; Peter Pomerantsev (2019) PublicAffairs \u0026ldquo;A Short History of Truth\u0026rdquo; Julian Baggini (2019) Quercus Publishing \u0026ldquo;Merchants of doubts\u0026rdquo; Naomi Oreskes, Erik M. Conway (2011) Bloomsbury Publishingng documentaries long \u0026ldquo;After Truth: Disinformation and the Cost of Fake News\u0026rdquo; (2020) \u0026ldquo;The Great Hack\u0026rdquo; (2019) \u0026ldquo;A Thousand Cuts\u0026rdquo; (2020) \u0026ldquo;Agents of Chaos\u0026rdquo; (2020) short \u0026ldquo;Operation Infektion\u0026rdquo; (3x15min, NY Times) (here) \u0026ldquo;Russian bot \u0026amp; trolls\u0026rdquo; (5min, NY Times) (here) \u0026ldquo;Fake: Searching for Truth in the Age of Misinformation\u0026rdquo; (2020) (\u0026lt; 1hour) (here) podcast “How they made us doubt everything” Peter Pomerantsev, BBC (2020) movies “Thank you for smoking” (2005) ","permalink":"https://chiaraaina.github.io/more/post_truth/","tags":null,"title":"post-truth \u0026 co."},{"categories":null,"contents":"\nreci.py I like writing recipes in minimal diagrams\npaintings for fun ","permalink":"https://chiaraaina.github.io/more/other/","tags":null,"title":"other"},{"categories":null,"contents":"Is it possible to persuade others only by providing interpretations of future events? I study the general problem of manipulating a boundedly rational agent by controlling her interpretation of signals she is about to receive. Persuasion arises because the agent updates her beliefs and takes an action based on the narrative she finds most plausible given her prior beliefs. Leveraging this, not only is the persuader able to strategically manipulate the agent to maximize his expected utility, but he can also induce the agent to hold non Bayes-plausible beliefs across signal realizations. Allowing for multiple stories to be communicated before the signal realizes, I propose a disciplined relaxation of the Bayes-plausibility constraint. This paper seeks to provide insights on the mechanism behind incoherent perceptions of the observed facts and the impact of persuasion via storytelling. I illustrate the model with applications in politics, finance, nudging policies, and self-control task.\n","permalink":"https://chiaraaina.github.io/publications/frustration_anger/","tags":null,"title":"Frustration and anger in the Ultimatum game: An experiment"},{"categories":null,"contents":"I study the problem of persuading a boundedly rational agent without controlling or knowing the piece of information she observes. Persuasion occurs by providing models whereby the persuader can communicate ways of interpreting observable signals. The key assumption is that the agent adopts the model that best fits what is observed, given her initial beliefs, and takes the action that maximizes her expected utility under the adopted model. I characterize the extent of belief manipulability in this setting and show that the agent may hold inconsistent beliefs across signal realizations \u0026mdash; posterior beliefs across realizations do not average to the prior \u0026mdash; because each signal may trigger the adoption of a different model. While persuasion can mislead the agent, the extent to which she is vulnerable to it is driven by her initial beliefs. Polarization is inevitable if agents with sufficiently different priors are exposed to the same conflicting models. I apply this framework to political polarization, conflict of interests in finance, lobbying, and self-persuasion.\n","permalink":"https://chiaraaina.github.io/research/tailored_stories/","tags":null,"title":"Tailored Stories"},{"categories":null,"contents":"We study the effect of contingent thinking on belief updating. One possible explanation for biased beliefs is that agents distort the underlying data generating process when updating their beliefs given a new piece of information. Engaging in contingent thinking — that is, reasoning through all possible contingencies without knowing which is realized — might affect the agents’ understanding of the data generating process, resulting in differences in belief updating. . Through a series of online experiments, we aim to uncover and break down the effect of contingent thinking on belief distortions into two components: (1) hypothetical thinking (updating on a piece of not-yet-observed information) and (2) contrast reasoning (comparing multiple contingencies during the updating task). Our project will proceed in three steps. First, we plan to establish the effects of contingent reasoning in a neutral task and investigate possible mechanisms by looking at heterogeneous treatment effects depending on the characteristics of the data generating process and individual characteristics. Second, we want to compare the impact of continent reasoning in other domains, specifically in an ego-relevant task. Third, we intend to examine whether contingent belief updating translates into changes in actions, and thus study the role of commitment in beliefs.\n","permalink":"https://chiaraaina.github.io/research/contingent/","tags":null,"title":"Contingent Belief Updating"},{"categories":null,"contents":"This project aims to understand how we update beliefs in the presence of competing narratives. It is common to be in situations where we are uncertain about how the data we observe was generated: different competing narratives could explain the data. How do people learn in these settings? Do they select only one narrative to update beliefs, or do they form posteriors that place non-zero weights on all narratives? Selecting only one narrative leads to more extreme beliefs, while weighting different narratives allows for more nuanced conclusions. In our experiment, we draw an exact correspondence with the theoretical literature on narratives that formalizes narratives as models (Schwartzstein and Sunderam, 2021). Our design allows isolating the weights subjects place on each model in order to compare them with theoretical predictions and categorize their behavior. More generally, we will explore the determinants of such weights — e.g., the model\u0026rsquo;s characteristics, the competing models\u0026rsquo; relative features, and individual traits. We are currently finalizing the design.\n","permalink":"https://chiaraaina.github.io/research/weighting/","tags":null,"title":"Weighting Competing Narratives"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://chiaraaina.github.io/search/","tags":null,"title":"Search Results"}]