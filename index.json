[{"categories":null,"contents":"Inspired by a lecture of Prof. Wei Ji Ma , I wrote this introduction to Bayesian updating for the course Behavioral Theory (University of Zurich, Fall 2019). What you know already about Bayes\u0026rsquo; rule Bayes\u0026rsquo; Theorem shows the relationship between a conditional probability and its counterpart. Consider two events $A $ and $ B $, Bayes\u0026rsquo; theorem states that, if $ P(B) \\ne 0 $,\n$$ P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}. $$\n$ P(A) $ and $ P(B) $ are the probabilities of observing $ A $ and $ B $. $ P(A \\mid B) $ is a conditional probability: the probability of event $ A $ occurring given that $ B $ occurs. $ P(B\\mid A) $ is also a conditional probability: the probability of event $ B $ occurring given that $ A $ occurs. Warm-up Exercise You come back from a vacation in an exotic place. You decide to go to the doctor to get tested for a disease common in that exotic area. But you know that medical results are not always accurate. Given the positive test result, what is the probability that you actually have this disease?\nTo formalize, denote the event that a patient has the disease as $D$ and the event the patient is healthy as $H$; let the event that the test turned out positive $+$ or negative $-$ for the disease. Bayes\u0026rsquo; Theorem helps you to compute the probability $ \\Pr(D \\mid +) $ given some other information: To determine the answer to this question you need to know: (i) incidence of the disease, that is the prior probability of the disease in the population, $ \\Pr(D)$ ; (ii) test accuracy:\nprobability of a false positive, $ \\Pr(+ \\mid H)$: how often does it report a positive result for someone without the disease? probability of a false negative, $ \\Pr(- \\mid D)$: how often does the test report a negative result for a sick patient? Assume $ \\Pr(D) = 10\\%, \\Pr(+ \\mid H) = 5\\%, $ and $ \\Pr(- \\mid D) = 20\\% $.\n(a) Draw in the figure the probabilities.\nsolution Priors\nTest Accuracy\nHow do you calculate the posterior probability of having the disease given a positive result of the test?\nUse Bayes\u0026rsquo; Theorem: $$ \\Pr(D \\mid + ) = \\frac{\\Pr(+ \\mid D) \\Pr(D)}{\\Pr(+)} !$$\nFirst, we determine the overall probability of a positive result (the denominator). To do so, we use the Law of Total Probability: for every event $ A $ and finite set of events $ B_n $, $ \\Pr(A) = \\sum_n \\Pr (A \\cap B_n) = \\sum_n \\Pr(A \\mid B_n) \\Pr(B_n)$.\n(b) Calculate the probability of a positive result and the probability of a negative result.\nsolution Use the Total Law of Probability: $\tP(+) = P(+ \\mid H) P(H) + P(+ \\mid D) P(D) = 5\\% \\times 90\\% + 80\\% \\times 10\\% = 12.5\\%. $ Then, $ \\ P(-) = 1 - 12.5\\% = 87.5\\% $.\nYou are ready to calculate the posterior probabilities.\n(c) Calculate the probability of being sick once observed a positive result, $ P(D \\mid +) $.\nsolution $$ \\Pr(D \\mid + )\t= \\frac{P(+ \\mid D) P(D)}{P(+)} = \\frac{80\\% \\times 10\\%}{12.5\\%} = 64\\% $$ There is a 64% chance that someone with a positive test is actually sick.\n(d) Which other probabilities can you recover from this? Which one you need to calculate by Bayes\u0026rsquo; Rule?\nsolution We can directly recover $\\Pr(H \\mid +) = 1- \\Pr(D \\mid + ) = 36\\%.$ One need to calculate the conditional probability on a negative test by Bayes\u0026rsquo; Rule.\n(e) Calculate the remaining conditional probabilities solution $$ \\Pr(D \\mid - ) = \\frac{\\Pr(- \\mid D) \\Pr(D)}{\\Pr(-)} = \\frac{20\\% \\times 10\\%}{ 87.5\\% } = 2.3\\% \\ \\Pr(H \\mid -) = 1 - 2.3\\% = 97.7 $$\nTo summarize:\nPosterior\rNegative\rPositive\rHealthy\r97.7%\r36%\rDisease\r2.3%\r64%\r(f) In the figure (source), assign the following label to the appropriate quantities: priors, test accuracy, probability of the results, and posteriors. solution Priors, $ \\Pr(D) $ and $ \\Pr(H) $: respectively, red dots and blue dots in the population; Test accuracy, $ \\Pr(+ \\mid H) $ and $ \\Pr(- \\mid H) $: respectively, blue bar and red bar in the test line; Probability of result, $ \\Pr(+) $ and $ \\Pr(-) $: respectively, all dots in the right column and all dots in the left column; Posteriors: $ \\Pr(D \\mid + ) $: all red dots in the right column; $ \\Pr(H \\mid + ) $: all blue dots in the right column; $ \\Pr(D \\mid - ) $: all red dots in the left column; $ \\Pr(H \\mid - ) $: all blue dots in the left column. More on Bayes\u0026rsquo; Theorem One of the most important application of Bayes\u0026rsquo; Theorem is inference: it allows us to put probability values on unknown parameter. Consider a random variable $x$ which is distributed conditional on an unknown parameter $ \\theta \\in \\Theta $. You observe some realizations $ x_{obs} $. Using these observations, we are interested in learning more about the true value of $ \\theta $. One interpretation is that this information tells you something about the model behind data.\nTo make this inference, you need to start from a prior distribution on the unknown parameter $\\theta$, that is $ \\Pr(\\theta) $ for each value $ \\theta \\in \\Theta $. This summarize the observer\u0026rsquo;s available information (or lack of information) about the parameter. Moreover, you need a likelihood function that tells you how to interpret the observations, absent prior knowledge. Formally, it is equal to the probability of the observed data under a parameter realization: $ \\Pr (x \\mid \\theta ).$\nExample of conditional distribution\nHowever, since the parameter is unknown and we can fix $ x $ using $ x_{obs} $, we treat the likelihood as a function of $ \\theta $: it expresses the plausibility of different parameter values for a given sample of data. A Bayesian observer computes a posterior distribution of $ \\theta $ given the observed value. In general, the posterior $\\Pr(\\theta \\mid x_{obs})$ is the probability density function over the unknown $\\theta$ given the observations $x_{obs}$, calculated by Bayes\u0026rsquo; rule as $$ \\Pr(\\theta \\mid x_{obs}) = \\frac{ \\Pr (x_{obs} \\mid \\theta)\\Pr(\\theta)}{\\Pr(x_{obs})} $$\nBayes\u0026rsquo; Theorem updates the information on $\\theta$ by extracting the information on $\\theta$ contained in the observation $x_{obs}$. So we can use this result to choose the best among a set of hypotheses as follows: each possible value of the parameter is a hypothesis, and the likelihood of that is the observer\u0026rsquo;s belief that the observations would arise under that hypothesis; priors and posteriors are both belief distributions whose arguments are hypothesis; we should select the hypothesis with the highest posterior to make our choices.\nLast, it is very useful to note that the posterior is proportional to the product of prior and likelihood (denominator is a normalization factor independent of $\\theta$): $$ \\Pr(\\theta \\mid x_{obs}) \\propto \\Pr (x_{obs} \\mid \\theta)\\Pr(\\theta). $$\nGo back to point (a) of the previous exercise. Draw the four products of prior and likelihood and check that it is suggestive of the final posterior.\nsolution Exercise: Competing hypotheses You observe bouncy balls all moving downward. You only consider two possible scenarios:\nScenario 1: all balls are part of the same object, thus they always move together. They move together either up or down, each with probability 0.5. Scenario 2: Each ball is an object by itself and independently moves either up or down, each with probability 0.5. Considering only four bouncy balls, you want to learn which of the following story is true. Note that in this example you only care about direction, whereas speed and position do not play a role in this problem.\nLikelihoods The likelihood of a scenario is the probability of the observations under that scenario: $ \\Pr( obs \\mid Scenario ).$\n(a) What is the likelihood of Scenario 1?\nsolution $\\Pr(obs \\mid Scenario 1) = 0.5 = 1/2.$\n(b) What is the likelihood of Scenario 2?\nsolution $ \\Pr(obs \\mid Scenario 2) = (0.5)^{4} = 1/16.$\n(c) Do the likelihoods of the scenarios sum to one?\nsolution No, there is no law of probability that says that probabilities of same observations under different scenarios sum up to one.\n(d) What is wrong with the phrase \u0026ldquo;the likelihood of the observations\u0026rdquo;?\nsolution One cannot speak about the likelihood of something without an considering a scenario to condition on. Remember that a likelihood is a conditional probability.\nPriors In many applications of interest, priors are not well-defined. In this examples, assume that Scenario 1 occurs twice as often as Scenario 2. You can use these frequencies of occurrence as prior probabilities, reflecting expectations in the absence of specific observations.\n(e) What are the prior probabilities of Scenario 1 and 2?\nsolution $ \\Pr(Scenario 1) = 2/3, \\quad \\Pr(Scenario 2)= 1/3. $\nNormalization Factor As an intermediate step, you need to calculate the product of the likelihood of a scenario multiplied by its prior probability: $ \\Pr(obs \\mid Scenario) \\times \\Pr(Scenario).$\nThese are used to calculate the numerator in Bayes\u0026rsquo; rule.\n(f) Calculate this product for Scenario 1 and 2?\nsolution $ \\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) = 2/3 \\times 1/2 = 1/3 $\n$ \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 ) = 1/3 \\times 1/16 = 1/48 $\n(g) Sum the two products. Is it equal to one?\nsolution No: $ \\quad \\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 ) = 1/3 + 1/48 = 17/48 $.\nThe normalization factor in Bayes\u0026rsquo; Rule is the overall probability of the observations and it is sum of the two products, as calculated above. This is because posteriors have to sum up to one.\nPosteriors (h) Divide the product of prior and likelihood by the normalization factor for each scenario\nsolution $$\t\\Pr(Scenario 1 \\mid obs) = \\frac{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) }{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 )} = \\frac{1/3}{17/48}=\\frac{1}{3} \\times \\frac{48}{17}= \\frac{16}{17}. $$\n$$ \\Pr(Scenario 2 \\mid obs) = \\frac{\\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario 2 ) }{\\Pr(obs \\mid Scenario 1) \\times \\Pr(Scenario1 ) + \\Pr(obs \\mid Scenario 2) \\times \\Pr(Scenario2 )} = \\frac{1/48}{17/48}= \\frac{1}{48} \\times \\frac{48}{17}= \\frac{1}{17}.$$\nYou have just applied Bayes\u0026rsquo; rule to obtain the posteriors for each scenario. Now you want to pick scenario with the highest posterior.\n(i) Which scenario has the highest posterior?\nsolution Scenario 1 is more likely given our observations. Indeed, the brain has a tendency to group the dots together because of their common motion, and perceive them as a single object. This is called Gestalt principle of common fate.\nYou have just explained a well-known phenomena by Bayes inference!\nExercise: Multiple Inputs Consider the following model: your utility of breakfast is given by the product of quality coffee and the quality of the cake you consume: $$ U(\\mbox{breakfast})= \\mbox{quality of cake} \\times \\mbox{quality of coffee}. $$ Assume that the quality is evaluated between 0 and 1.\nThis morning you had a very bad breakfast, with utility of breakfast today 0.2. You want to understand the cause of that. You are better at assessing separately the quality of coffee, but not the quality of cake.\n(a) Suppose further that you hypothesized the quality of the coffee to be 1 (very good coffee). Under this hypothesis, calculate what the quality of the cake must have been.\nsolution $$ \\underbrace{U(\\mbox{breakfast})}_{0.2} = \\mbox{quality of cake} \\times \\underbrace{\\mbox{quality of coffee}}_1 $$\nSo the quality of cake must have been 0.2.\n(b) What if you suppose that the quality of coffee to be 0.4?\nsolution $$ \\underbrace{U(\\mbox{breakfast})}_{0.2} = \\mbox{quality of cake} \\times \\underbrace{\\mbox{quality of coffee}}_{0.4} $$\rSo the quality of cake must have been 0.5.\n(c) Why utility of breakfast provides ambiguous information about quality of the cake?\nsolution Because quality of coffee modulates the utility of breakfast together with the quality of cake and you do not have accurate information on that.\n(d) By going through a few more examples like the ones in (a) and (b), draw on the two-variable likelihood diagram all combinations of hypothesized quality of coffee and hypothesized quality of cake that could have produced the utility of breakfast equal to 0.2. solution (e) Suppose that you have a strong prior that quality of coffee is uniformly distributed between 0.2 and 0.4 for sure. In the two-variable prior diagram, shade the area corresponding to the support of the prior.\nsolution (f) Can we infer something about the quality of cake? Finally draw where the posterior probability is high. solution You can just restricted your posterior. If you believe that the quality of coffee is between 0.2 and 0.5, it means that the quality of cake was above 0.5: the cake was good!\nPuzzle: Monty Hall Paradox You\u0026rsquo;re on a TV show. You\u0026rsquo;re given the choice of three doors: behind one door is a car (prize); behind the others, goats (non-prize). Which one would you pick? Suppose you pick door No.1. Now the host, who knows what\u0026rsquo;s behind the doors, does not open the one you selected but opens another door, say No. 3, which has a goat. Then the host says to you, ``Do you want to pick door No. 2?\u0026quot;\nDoes it matter if you switch? Yes! This is a famous counter-intuitive problem. Let\u0026rsquo;s use Bayes\u0026rsquo; Rule to solve it.\nDenote the event \u0026ldquo;door $ i $ hides a car\u0026rdquo; with $ C_i $.\n(a) When the host of the TV show gives you the choice to pick a door, what is your prior probability of finding the car behind each door?\nsolution Since you do not have any information, you believe each door is equally likely to hide the car: $ \\Pr(C_1) = \\Pr(C_2) = \\Pr(C_3) = \\frac{1}{3}.$\nDenote the event \u0026ldquo;door $ i $ is chosen by the guest\u0026rdquo; with $ X_i $ and the event \u0026ldquo;door $ i $ is opened\u0026rdquo; with $ O_i $. Your priors are unchanged after the host opens a door, by independence between the event $ C_i $ and $ X_i $ (the player doesn\u0026rsquo;t know where is the car in order to make a choice). This implies that $ \\Pr(C_i, X_i) = \\Pr(C_i) \\times \\Pr(X_i)$.\nInitially you have chosen door No. 1. Now the host will open a door that does not hide the car.\n(b) What is the probability that the host will open door No. 3 in the following cases?\nsolution The car is behind door No. 1: $ \\Pr(O_3 \\mid X_1, C_1) = \\frac{1}{2}.$ The car is behind door No. 2: $ \\Pr(O_3 \\mid X_1, C_2) = 1.$ The car is behind door No. 3: $ \\Pr(O_3 \\mid X_1, C_3) = 0.$ (c) What is the overall probability that the host will open door no.3?\nsolution Using total law of probability: $$ \\Pr(O_3 \\mid X_1) = \\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1) + \\Pr(O_3 \\mid X_1, C_2) \\times \\Pr(C_2) + \\Pr(O_3 \\mid X_1, C_3) \\times \\Pr(C_3) = \\frac{1}{2} \\times \\frac{1}{3} + 1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} = \\frac{1}{2}.$$\nThe host opens door No.3. Now we know that $ \\Pr(C_3 \\mid 0_3) = 0 $ (posterior probability).\n(d) Use Bayes\u0026rsquo; rule to calculate the conditional probability that the car is behind the door you chose, given that door No.3 was open: $ \\Pr(C_1 \\mid X_1, O_3) $. solution $$\\Pr(C_1 \\mid X_1, O_3) = \\frac{\\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1, X_1)}{\\Pr(X_1, O_3)} = \\frac{\\Pr(O_3 \\mid X_1, C_1) \\times \\Pr(C_1) \\times \\Pr(X_1)}{\\Pr(O_3 \\mid X_1)\\times \\Pr(X_1)} = \\frac{1/2 \\times 1/3 \\times 1}{1/2 \\times 1} = \\frac{1}{3} $$\n(e) Calculate the conditional probability that the car is behind the other door.\nsolution This is just the complement of $\\Pr(C_2 \\mid X_1, O_3) $, since door No.3 was open.\n$$\\Pr(C_2 \\mid X_1, O_3) = 1- \\Pr(C_2 \\mid X_1, O_3) = \\frac{2}{3} $$\n(f) Should you switch door?\nsolution Yes!\n","permalink":"https://chiaraaina.github.io/more/bayesian_updating/","tags":null,"title":"bayesian updating"},{"categories":null,"contents":"a bunch of resources about post-truth that I read, watched, listened\nbooks \u0026ldquo;Nothing is true, everything is possible\u0026rdquo; Peter Pomerantsev (2015) PublicAffairs \u0026ldquo;A Lot of People Are Saying\u0026rdquo; Nancy L. Rosenblum, Russell Muirhead (2019) Princeton University Press \u0026ldquo;Post-Truth\u0026rdquo; Lee Mcintyre (2018) MIT Press \u0026ldquo;This is not propaganda\u0026rdquo; Peter Pomerantsev (2019) PublicAffairs \u0026ldquo;A Short History of Truth\u0026rdquo; Julian Baggini (2019) Quercus Publishing \u0026ldquo;Merchants of doubts\u0026rdquo; Naomi Oreskes, Erik M. Conway (2011) Bloomsbury Publishingng documentaries long \u0026ldquo;After Truth: Disinformation and the Cost of Fake News\u0026rdquo; (2020) \u0026ldquo;The Great Hack\u0026rdquo; (2019) \u0026ldquo;A Thousand Cuts\u0026rdquo; (2020) \u0026ldquo;Agents of Chaos\u0026rdquo; (2020) short \u0026ldquo;Operation Infektion\u0026rdquo; (3x15min, NY Times) (here) \u0026ldquo;Russian bot \u0026amp; trolls\u0026rdquo; (5min, NY Times) (here) \u0026ldquo;Fake: Searching for Truth in the Age of Misinformation\u0026rdquo; (2020) (\u0026lt; 1hour) (here) podcast “How they made us doubt everything” Peter Pomerantsev, BBC (2020) movies “Thank you for smoking” (2005) ","permalink":"https://chiaraaina.github.io/more/post_truth/","tags":null,"title":"post-truth \u0026 co."},{"categories":null,"contents":"\nreci.py I like writing recipes in minimal diagrams\npaintings for fun ","permalink":"https://chiaraaina.github.io/more/other/","tags":null,"title":"other"},{"categories":null,"contents":"Is it possible to persuade others only by providing interpretations of future events? I study the general problem of manipulating a boundedly rational agent by controlling her interpretation of signals she is about to receive. Persuasion arises because the agent updates her beliefs and takes an action based on the narrative she finds most plausible given her prior beliefs. Leveraging this, not only is the persuader able to strategically manipulate the agent to maximize his expected utility, but he can also induce the agent to hold non Bayes-plausible beliefs across signal realizations. Allowing for multiple stories to be communicated before the signal realizes, I propose a disciplined relaxation of the Bayes-plausibility constraint. This paper seeks to provide insights on the mechanism behind incoherent perceptions of the observed facts and the impact of persuasion via storytelling. I illustrate the model with applications in politics, finance, nudging policies, and self-control task.\n","permalink":"https://chiaraaina.github.io/publications/frustration_anger/","tags":null,"title":"Frustration and Anger in the Ultimatum game: An Experiment"},{"categories":null,"contents":"To what extent is it possible to manipulate beliefs by providing interpretations of unknown events? I characterize the feasible posteriors across signals when the agent is exposed to a set of models to interpret observable signals and adopts the model that best fits what is observed. Because each signal could trigger the adoption of a different model, posteriors across signal realizations might not average to the prior. The scope of persuasion is large, even for a persuader who does not control or know the signal the agent observes. I apply this framework to political polarization, finance, lobbying, and self-persuasion.\n","permalink":"https://chiaraaina.github.io/research/tailored_stories/","tags":null,"title":"Tailored Stories"},{"categories":null,"contents":"We study the impact of contingent thinking on belief updating. Engaging in contingent thinking calls for both processing hypothetical information and contrasting multiple contingencies during the belief-updating process. Our experimental findings show that contingent thinking leads to significant deviations from Bayesian updating. These deviations arise from the diminished perceived informativeness of hypothetical signals and the challenges posed by asymmetric signals, where comparing contingencies becomes more difficult. These results have implications for contingent planning, information acquisition, and information design.\n","permalink":"https://chiaraaina.github.io/research/contingent/","tags":null,"title":"Contingent Belief Updating"},{"categories":null,"contents":"This project aims to understand how we update beliefs in the presence of competing models. It is common to be in situations where different models could explain the data. How do people learn from data in these situations? Do they select only one model to update beliefs, or do they form posteriors by weighting the predictions of the competing models? Our experimental design allows us to isolate the weights subjects place on different models and to compare them with the theoretical predictions. We find that a substantial share of people solely adopt the model that explains the data best, in line with Maximum likelihood selection. However, Bayesian updating would require weighting models: we find that many subjects do so, but with distorted weights compared to the Bayesian benchmark. Shedding light on this topic is essential to uncover whether people can keep multiple models in mind and avoid extreme conclusions due to the adoption of a single model.\n","permalink":"https://chiaraaina.github.io/research/weighting/","tags":null,"title":"Weighting Competing Models"},{"categories":null,"contents":"We study the competition among senders in the supply of models. For example, politicians compete to supply voters with models to interpret data on immigration or economic outcomes; brokers might want clients to interpret the same firm’s performance differently, driven by different incentives. The senders\u0026rsquo; strategies and the receiver\u0026rsquo;s beliefs vary with respect to the incentives of the parties involved\n","permalink":"https://chiaraaina.github.io/research/competing/","tags":null,"title":"Competing in Tailores Stories"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://chiaraaina.github.io/search/","tags":null,"title":"Search Results"}]