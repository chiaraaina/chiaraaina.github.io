<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research on Chiara Aina</title>
    <link>https://chiaraaina.github.io/research/</link>
    <description>Recent content in research on Chiara Aina</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2020 16:47:30 +0000</lastBuildDate><atom:link href="https://chiaraaina.github.io/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Contingent Belief Updating</title>
      <link>https://chiaraaina.github.io/research/contingent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chiaraaina.github.io/research/contingent/</guid>
      <description>We study the effect of contingent thinking on belief updating. One possible explanation for biased beliefs is that agents distort the underlying data generating process when updating their beliefs given a new piece of information. Engaging in contingent thinking — that is, reasoning through all possible contingencies without knowing which is realized — might affect the agents’ understanding of the data generating process, resulting in differences in belief updating. . Through a series of online experiments, we aim to uncover and break down the effect of contingent thinking on belief distortions into two components: (1) hypothetical thinking (updating on a piece of not-yet-observed information) and (2) contrast reasoning (comparing multiple contingencies during the updating task).</description>
    </item>
    
    <item>
      <title>Weighting Competing Narratives</title>
      <link>https://chiaraaina.github.io/research/weighting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chiaraaina.github.io/research/weighting/</guid>
      <description>This project aims to understand how we update beliefs in the presence of competing narratives. It is common to be in situations where we are uncertain about how the data we observe was generated: different competing narratives could explain the data. How do people learn in these settings? Do they select only one narrative to update beliefs, or do they form posteriors that place non-zero weights on all narratives? Selecting only one narrative leads to more extreme beliefs, while weighting different narratives allows for more nuanced conclusions.</description>
    </item>
    
  </channel>
</rss>
